{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9J7SOSk4VrAqW/12J0mOj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FolarinJoaquim/ADHD-Project-Implementation/blob/main/Subreddit_Scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/FolarinJoaquim/ADHD-Project-Implementation.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4HmexFx8rLm",
        "outputId": "134e9227-a0a5-442d-9cfb-5b99801155ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ADHD-Project-Implementation'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 11 (delta 3), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (11/11), 5.07 KiB | 5.07 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All our imports\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n"
      ],
      "metadata": {
        "id": "-He_y7d08HhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1FCZ4Ie680x"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function -> Main Scraper to get content from Reddit\n",
        "def scrape_reddit() -> list[dict]:\n",
        "    subreddits = [\n",
        "        \"https://www.reddit.com/r/Python\",\n",
        "        \"https://www.reddit.com/r/Programming\",\n",
        "        \"https://www.reddit.com/r/learnpython\",\n",
        "    ]\n",
        "\n",
        "    all_data = []\n",
        "    for url in subreddits:\n",
        "        headers = {\n",
        "              'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "\n",
        "        subreddit_name = url.split(\"/\")[-1]\n",
        "        print(f'Scraping for: {subreddit_name}')\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "            subreddit_data ={\n",
        "                \"subreddit\" : subreddit_name,\n",
        "                \"url\" : url,\n",
        "                \"title\": soup.title.string if soup.title else \"No title\",\n",
        "                \"scraped_at\" : time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            }\n",
        "\n",
        "            topics = []\n",
        "\n",
        "            for heading in soup.find_all([\"h1\",\"h2\",\"h3\",\"h4\"]):\n",
        "                text = heading.get_text(strip=True)\n",
        "\n",
        "                if text and len(text) > 3:\n",
        "                    if any(keyword in text.lower() for keyword in [\"python\",\"programming\",\"code\",\"develop\",\"coding\"]):\n",
        "                        topics.append({\n",
        "                            \"title\":text,\n",
        "                            \"type\": \"python_topic\"\n",
        "                        })\n",
        "\n",
        "            discussions = []\n",
        "            seen_urls = set()\n",
        "\n",
        "            for link in soup.find_all(\"a\", href=True):\n",
        "                text = link.get_text(strip=True)\n",
        "                href = link[\"href\"]\n",
        "\n",
        "                if text and len(text) > 1 and \"/comments/\" in href and href not in seen_urls:\n",
        "                    seen_urls.add(href)\n",
        "\n",
        "                    discussions.append({\n",
        "                        \"title\": text[:100] + \"...\" if len(text) > 100 else text,\n",
        "                        \"url\":href,\n",
        "                        \"type\":\"discussion\"\n",
        "                    })\n",
        "\n",
        "            subreddit_data[\"python_topics\"] = topics\n",
        "            subreddit_data[\"discussions\"] = discussions\n",
        "\n",
        "            all_data.append(subreddit_data)\n",
        "            time.sleep(2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'ERROR: {e}')\n",
        "\n",
        "    return all_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function -> Saved scraped data (Both json and csv)\n",
        "def save_scraped_data(data, filename_json=\"python_topics.json\", filename_csv=\"python_topics.csv\"):\n",
        "    if not data:\n",
        "        print(f'No data')\n",
        "        return\n",
        "\n",
        "    # JSON\n",
        "    try:\n",
        "        with open(filename_json, \"w\", encoding=\"utf-8\") as file:\n",
        "            json.dump(data, file, indent=2, ensure_ascii=True)\n",
        "        print(f'Python Topics saved: {filename_json}')\n",
        "    except Exception as e:\n",
        "        print(\"ERROR:\",e)\n",
        "\n",
        "\n",
        "    # CSV\n",
        "    try:\n",
        "        with open(filename_csv, 'w', newline='', encoding='utf-8') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow(['Subreddit', 'Type', 'Title', 'URL', 'Scraped At'])\n",
        "\n",
        "            for subreddit_data in data:\n",
        "                subreddit = subreddit_data['subreddit']\n",
        "                scraped_at = subreddit_data['scraped_at']\n",
        "\n",
        "                for topic in subreddit_data.get('python_topics', []):\n",
        "                    writer.writerow([subreddit, topic['type'], topic['title'], '', scraped_at])\n",
        "\n",
        "                for discussion in subreddit_data.get('discussions', []):\n",
        "                    writer.writerow([subreddit, discussion['type'], discussion['title'], discussion['url'], scraped_at])\n",
        "\n",
        "            print(f'All topics saved to files!')\n",
        "    except Exception as e:\n",
        "        print(\"Error:\",e)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Main -> Execute the code above\n",
        "def main() -> None:\n",
        "    data = scrape_reddit()\n",
        "\n",
        "    if data:\n",
        "        print(f'Processing the data...')\n",
        "        total_topics = 0\n",
        "        total_discussions = 0\n",
        "\n",
        "        for subreddit_data in data:\n",
        "            topics_count = len(subreddit_data.get(\"python_topics\",[]))\n",
        "            discussions_count = len(subreddit_data.get(\"discussions\",[]))\n",
        "\n",
        "            total_topics += topics_count\n",
        "            total_discussions += discussions_count\n",
        "\n",
        "        print(f'\\nTotal: {total_topics}: Python Topics, {discussions_count} Discussions')\n",
        "\n",
        "        save_scraped_data(data)\n",
        "    else:\n",
        "        print(\"There is not data returned!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}